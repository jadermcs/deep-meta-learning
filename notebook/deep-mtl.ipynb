{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "variable-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Optional, Any\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protecting-metropolitan",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- implement selfsupervised as in [TabNet](https://arxiv.org/pdf/1908.07442.pdf)\n",
    "- implement mask?\n",
    "- pad datasets to same size and average all outputs and compute the regression on them, like  [this](https://keras.io/examples/nlp/text_classification_with_transformer/) OR the BERT approach, like [this](https://stackoverflow.com/questions/58123393/how-to-use-transformers-for-text-classification)\n",
    "- regress for bounded target like [this](https://stats.stackexchange.com/questions/11985/how-to-model-bounded-target-variable) or [this](https://stackoverflow.com/questions/51693567/best-way-to-bound-outputs-from-neural-networks-on-reinforcement-learning)\n",
    "- loss crossentropy or mse?\n",
    "- add more algorithms for regression with multitask\n",
    "- finetune for tree depth, svm kernel, etc\n",
    "- selfsupervise for data imputation (like in TabNet)\n",
    "- finetune for best pre-processing pipeline\n",
    "- inspect attention plots(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "informative-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "requested-singapore",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1694' class='' max='1694' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1694/1694 00:03<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"../samples_train/\"\n",
    "files = os.listdir(path)\n",
    "for f in progress_bar(files):\n",
    "    df = pd.read_csv(path+f)\n",
    "    data = df.values.astype(float).T\n",
    "    target = float(f.split('_')[-2])\n",
    "    X.append(data)\n",
    "    y.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "located-desert",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "perfect-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "foster-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, nhead, dim_feedforward=256, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        self.activation = torch.nn.functional.relu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        src2 = self.self_attn(src, src, src, attn_mask=src_mask)[0]\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        return src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "modern-anchor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionMetaExtractor(nn.Module):\n",
    "    \n",
    "    def __init__(self, ninp, noutput, nhead=5, nhid=256, nlayers=10, dropout=.25):\n",
    "        super(AttentionMetaExtractor, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "#         self.norm = nn.BatchNorm1d(ninp, affine=False,     # no learnable parameters as different ..\n",
    "#                                track_running_stats=False) # .. batchs means different data.\n",
    "        encoder_block = Encoder(ninp, nhead, nhid)\n",
    "        self.encoder = nn.ModuleList([copy.deepcopy(encoder_block) for _ in range(nlayers)])\n",
    "        self.decoder = nn.Linear(ninp, nhid)\n",
    "        self.ninp = ninp\n",
    "        self.output = nn.Linear(nhid, noutput)\n",
    "        \n",
    "    def forward(self, src: torch.Tensor) -> torch.Tensor:\n",
    "#         src = self.norm(src) # data already normalized\n",
    "        output = src * math.sqrt(self.ninp)\n",
    "        for block in self.encoder:\n",
    "            output = block(output)\n",
    "        output = torch.mean(output, dim = 1)\n",
    "        output = self.decoder(output)\n",
    "        output = self.output(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "unlikely-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ninp = 128 # number of rows in base data\n",
    "nhead = 8\n",
    "noutput = 1 # number of algorithms accuracies being regressed\n",
    "nhid = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "atomic-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionMetaExtractor(ninp, noutput, nhead, nhid).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "crucial-gallery",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AttentionMetaExtractor(\n",
       "  (encoder): ModuleList(\n",
       "    (0): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): Encoder(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pleasant-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(i, data, target):\n",
    "    data = torch.tensor(data[i]).unsqueeze(0).to(device).float()\n",
    "    target = torch.tensor(target[i]).reshape(-1).to(device).float()\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "under-adelaide",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.5977, -0.8230, -0.7769,  ...,  0.0525, -0.6527,  0.5745],\n",
      "         [ 0.1358, -0.5994, -0.4055,  ..., -0.3641, -0.2863,  0.3330],\n",
      "         [-0.1603, -0.5288, -0.2194,  ...,  0.1150,  0.2108, -0.5212],\n",
      "         ...,\n",
      "         [-0.6785,  0.6437,  0.8022,  ...,  0.8378, -0.2926,  0.3217],\n",
      "         [-0.7252,  0.8357,  0.3426,  ...,  0.8005, -0.0738,  0.3687],\n",
      "         [-0.1428,  0.4012, -0.9255,  ..., -1.0973,  0.0720,  0.2380]]],\n",
      "       device='cuda:0') torch.Size([1, 10, 128])\n",
      "tensor([0.8680], device='cuda:0') torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "a, b = get_batch(101, xtrain, ytrain)\n",
    "print(a, a.shape)\n",
    "print(b, b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "integrated-citizen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1113]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "res = model(a)\n",
    "print(res)\n",
    "# print(torch.mean(res, 2), torch.mean(res, 2).shape)\n",
    "# print(torch.std(res, 2), torch.std(res, 2).shape)\n",
    "# print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "equipped-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.01 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "classical-actress",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    for batch, i in enumerate(range(len(xtrain))):\n",
    "        data, targets = get_batch(i, xtrain, ytrain)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)[0]\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 100\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.3f} | ppl {:8.3f}'.format(\n",
    "                    epoch, batch, len(xtrain), scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "            start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "controlling-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = xtrain # TODO: alterar treino teste\n",
    "\n",
    "def evaluate(eval_model):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(xvalid)): \n",
    "            data, targets = get_batch(i, xvalid, yvalid)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output[0]\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(xtest) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "imposed-being",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jader/Projects/deep-meta-learning/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:369: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   100/ 1270 batches | lr 0.00 | ms/batch 33.04 | loss 0.533 | ppl    1.704\n",
      "| epoch   1 |   200/ 1270 batches | lr 0.00 | ms/batch 32.58 | loss 0.255 | ppl    1.290\n",
      "| epoch   1 |   300/ 1270 batches | lr 0.00 | ms/batch 31.91 | loss 0.087 | ppl    1.091\n",
      "| epoch   1 |   400/ 1270 batches | lr 0.00 | ms/batch 31.18 | loss 0.038 | ppl    1.039\n",
      "| epoch   1 |   500/ 1270 batches | lr 0.00 | ms/batch 30.79 | loss 0.026 | ppl    1.026\n",
      "| epoch   1 |   600/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.030 | ppl    1.030\n",
      "| epoch   1 |   700/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.029 | ppl    1.030\n",
      "| epoch   1 |   800/ 1270 batches | lr 0.00 | ms/batch 31.19 | loss 0.023 | ppl    1.024\n",
      "| epoch   1 |   900/ 1270 batches | lr 0.00 | ms/batch 31.03 | loss 0.031 | ppl    1.031\n",
      "| epoch   1 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.024 | ppl    1.025\n",
      "| epoch   1 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.56 | loss 0.028 | ppl    1.028\n",
      "| epoch   1 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.05 | loss 0.019 | ppl    1.019\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 42.55s | valid loss 0.008 | valid ppl    1.008\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   100/ 1270 batches | lr 0.00 | ms/batch 32.15 | loss 0.028 | ppl    1.028\n",
      "| epoch   2 |   200/ 1270 batches | lr 0.00 | ms/batch 30.99 | loss 0.026 | ppl    1.026\n",
      "| epoch   2 |   300/ 1270 batches | lr 0.00 | ms/batch 30.84 | loss 0.022 | ppl    1.022\n",
      "| epoch   2 |   400/ 1270 batches | lr 0.00 | ms/batch 31.24 | loss 0.027 | ppl    1.027\n",
      "| epoch   2 |   500/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.021 | ppl    1.021\n",
      "| epoch   2 |   600/ 1270 batches | lr 0.00 | ms/batch 31.44 | loss 0.026 | ppl    1.027\n",
      "| epoch   2 |   700/ 1270 batches | lr 0.00 | ms/batch 31.55 | loss 0.028 | ppl    1.028\n",
      "| epoch   2 |   800/ 1270 batches | lr 0.00 | ms/batch 31.29 | loss 0.021 | ppl    1.021\n",
      "| epoch   2 |   900/ 1270 batches | lr 0.00 | ms/batch 31.32 | loss 0.025 | ppl    1.025\n",
      "| epoch   2 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.43 | loss 0.026 | ppl    1.026\n",
      "| epoch   2 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.12 | loss 0.024 | ppl    1.024\n",
      "| epoch   2 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.88 | loss 0.017 | ppl    1.017\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 42.48s | valid loss 0.008 | valid ppl    1.008\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   100/ 1270 batches | lr 0.00 | ms/batch 31.81 | loss 0.026 | ppl    1.026\n",
      "| epoch   3 |   200/ 1270 batches | lr 0.00 | ms/batch 31.32 | loss 0.025 | ppl    1.025\n",
      "| epoch   3 |   300/ 1270 batches | lr 0.00 | ms/batch 31.25 | loss 0.021 | ppl    1.022\n",
      "| epoch   3 |   400/ 1270 batches | lr 0.00 | ms/batch 31.32 | loss 0.024 | ppl    1.024\n",
      "| epoch   3 |   500/ 1270 batches | lr 0.00 | ms/batch 31.50 | loss 0.019 | ppl    1.019\n",
      "| epoch   3 |   600/ 1270 batches | lr 0.00 | ms/batch 31.45 | loss 0.024 | ppl    1.024\n",
      "| epoch   3 |   700/ 1270 batches | lr 0.00 | ms/batch 31.39 | loss 0.025 | ppl    1.025\n",
      "| epoch   3 |   800/ 1270 batches | lr 0.00 | ms/batch 31.75 | loss 0.018 | ppl    1.019\n",
      "| epoch   3 |   900/ 1270 batches | lr 0.00 | ms/batch 31.88 | loss 0.023 | ppl    1.023\n",
      "| epoch   3 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.47 | loss 0.024 | ppl    1.024\n",
      "| epoch   3 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.48 | loss 0.023 | ppl    1.024\n",
      "| epoch   3 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.51 | loss 0.015 | ppl    1.015\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 42.63s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   100/ 1270 batches | lr 0.00 | ms/batch 31.64 | loss 0.027 | ppl    1.027\n",
      "| epoch   4 |   200/ 1270 batches | lr 0.00 | ms/batch 31.99 | loss 0.024 | ppl    1.025\n",
      "| epoch   4 |   300/ 1270 batches | lr 0.00 | ms/batch 32.82 | loss 0.023 | ppl    1.023\n",
      "| epoch   4 |   400/ 1270 batches | lr 0.00 | ms/batch 31.78 | loss 0.025 | ppl    1.025\n",
      "| epoch   4 |   500/ 1270 batches | lr 0.00 | ms/batch 31.37 | loss 0.019 | ppl    1.019\n",
      "| epoch   4 |   600/ 1270 batches | lr 0.00 | ms/batch 31.02 | loss 0.023 | ppl    1.023\n",
      "| epoch   4 |   700/ 1270 batches | lr 0.00 | ms/batch 31.27 | loss 0.022 | ppl    1.022\n",
      "| epoch   4 |   800/ 1270 batches | lr 0.00 | ms/batch 30.95 | loss 0.019 | ppl    1.019\n",
      "| epoch   4 |   900/ 1270 batches | lr 0.00 | ms/batch 31.43 | loss 0.023 | ppl    1.023\n",
      "| epoch   4 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.020 | ppl    1.020\n",
      "| epoch   4 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.95 | loss 0.022 | ppl    1.022\n",
      "| epoch   4 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.19 | loss 0.014 | ppl    1.015\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 42.46s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   100/ 1270 batches | lr 0.00 | ms/batch 31.83 | loss 0.024 | ppl    1.024\n",
      "| epoch   5 |   200/ 1270 batches | lr 0.00 | ms/batch 31.41 | loss 0.023 | ppl    1.023\n",
      "| epoch   5 |   300/ 1270 batches | lr 0.00 | ms/batch 31.40 | loss 0.021 | ppl    1.021\n",
      "| epoch   5 |   400/ 1270 batches | lr 0.00 | ms/batch 31.10 | loss 0.023 | ppl    1.023\n",
      "| epoch   5 |   500/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.018 | ppl    1.018\n",
      "| epoch   5 |   600/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.022 | ppl    1.022\n",
      "| epoch   5 |   700/ 1270 batches | lr 0.00 | ms/batch 31.10 | loss 0.022 | ppl    1.022\n",
      "| epoch   5 |   800/ 1270 batches | lr 0.00 | ms/batch 31.17 | loss 0.017 | ppl    1.017\n",
      "| epoch   5 |   900/ 1270 batches | lr 0.00 | ms/batch 31.12 | loss 0.023 | ppl    1.023\n",
      "| epoch   5 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.62 | loss 0.020 | ppl    1.021\n",
      "| epoch   5 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.10 | loss 0.019 | ppl    1.019\n",
      "| epoch   5 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.26 | loss 0.014 | ppl    1.014\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 42.22s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   100/ 1270 batches | lr 0.00 | ms/batch 31.53 | loss 0.025 | ppl    1.026\n",
      "| epoch   6 |   200/ 1270 batches | lr 0.00 | ms/batch 31.35 | loss 0.024 | ppl    1.024\n",
      "| epoch   6 |   300/ 1270 batches | lr 0.00 | ms/batch 31.60 | loss 0.021 | ppl    1.021\n",
      "| epoch   6 |   400/ 1270 batches | lr 0.00 | ms/batch 31.41 | loss 0.024 | ppl    1.024\n",
      "| epoch   6 |   500/ 1270 batches | lr 0.00 | ms/batch 31.26 | loss 0.019 | ppl    1.019\n",
      "| epoch   6 |   600/ 1270 batches | lr 0.00 | ms/batch 31.35 | loss 0.022 | ppl    1.022\n",
      "| epoch   6 |   700/ 1270 batches | lr 0.00 | ms/batch 31.21 | loss 0.020 | ppl    1.020\n",
      "| epoch   6 |   800/ 1270 batches | lr 0.00 | ms/batch 30.93 | loss 0.017 | ppl    1.017\n",
      "| epoch   6 |   900/ 1270 batches | lr 0.00 | ms/batch 31.13 | loss 0.021 | ppl    1.021\n",
      "| epoch   6 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.86 | loss 0.020 | ppl    1.020\n",
      "| epoch   6 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.96 | loss 0.020 | ppl    1.020\n",
      "| epoch   6 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.71 | loss 0.014 | ppl    1.014\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 42.17s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   100/ 1270 batches | lr 0.00 | ms/batch 31.83 | loss 0.025 | ppl    1.025\n",
      "| epoch   7 |   200/ 1270 batches | lr 0.00 | ms/batch 31.42 | loss 0.023 | ppl    1.023\n",
      "| epoch   7 |   300/ 1270 batches | lr 0.00 | ms/batch 31.46 | loss 0.020 | ppl    1.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   7 |   400/ 1270 batches | lr 0.00 | ms/batch 30.81 | loss 0.022 | ppl    1.022\n",
      "| epoch   7 |   500/ 1270 batches | lr 0.00 | ms/batch 31.71 | loss 0.017 | ppl    1.018\n",
      "| epoch   7 |   600/ 1270 batches | lr 0.00 | ms/batch 31.32 | loss 0.018 | ppl    1.018\n",
      "| epoch   7 |   700/ 1270 batches | lr 0.00 | ms/batch 31.60 | loss 0.019 | ppl    1.019\n",
      "| epoch   7 |   800/ 1270 batches | lr 0.00 | ms/batch 31.18 | loss 0.017 | ppl    1.017\n",
      "| epoch   7 |   900/ 1270 batches | lr 0.00 | ms/batch 31.83 | loss 0.021 | ppl    1.021\n",
      "| epoch   7 |  1000/ 1270 batches | lr 0.00 | ms/batch 34.32 | loss 0.022 | ppl    1.022\n",
      "| epoch   7 |  1100/ 1270 batches | lr 0.00 | ms/batch 32.14 | loss 0.021 | ppl    1.021\n",
      "| epoch   7 |  1200/ 1270 batches | lr 0.00 | ms/batch 32.21 | loss 0.015 | ppl    1.015\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 43.36s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   100/ 1270 batches | lr 0.00 | ms/batch 32.35 | loss 0.023 | ppl    1.023\n",
      "| epoch   8 |   200/ 1270 batches | lr 0.00 | ms/batch 31.21 | loss 0.022 | ppl    1.022\n",
      "| epoch   8 |   300/ 1270 batches | lr 0.00 | ms/batch 31.68 | loss 0.019 | ppl    1.019\n",
      "| epoch   8 |   400/ 1270 batches | lr 0.00 | ms/batch 30.89 | loss 0.020 | ppl    1.020\n",
      "| epoch   8 |   500/ 1270 batches | lr 0.00 | ms/batch 30.96 | loss 0.019 | ppl    1.019\n",
      "| epoch   8 |   600/ 1270 batches | lr 0.00 | ms/batch 30.73 | loss 0.020 | ppl    1.020\n",
      "| epoch   8 |   700/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.019 | ppl    1.019\n",
      "| epoch   8 |   800/ 1270 batches | lr 0.00 | ms/batch 31.17 | loss 0.015 | ppl    1.015\n",
      "| epoch   8 |   900/ 1270 batches | lr 0.00 | ms/batch 30.94 | loss 0.020 | ppl    1.020\n",
      "| epoch   8 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.023 | ppl    1.023\n",
      "| epoch   8 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.37 | loss 0.021 | ppl    1.022\n",
      "| epoch   8 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.23 | loss 0.014 | ppl    1.014\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 42.20s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   100/ 1270 batches | lr 0.00 | ms/batch 31.62 | loss 0.022 | ppl    1.023\n",
      "| epoch   9 |   200/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.023 | ppl    1.023\n",
      "| epoch   9 |   300/ 1270 batches | lr 0.00 | ms/batch 31.76 | loss 0.021 | ppl    1.021\n",
      "| epoch   9 |   400/ 1270 batches | lr 0.00 | ms/batch 31.29 | loss 0.021 | ppl    1.021\n",
      "| epoch   9 |   500/ 1270 batches | lr 0.00 | ms/batch 31.30 | loss 0.017 | ppl    1.018\n",
      "| epoch   9 |   600/ 1270 batches | lr 0.00 | ms/batch 31.19 | loss 0.019 | ppl    1.019\n",
      "| epoch   9 |   700/ 1270 batches | lr 0.00 | ms/batch 31.58 | loss 0.018 | ppl    1.018\n",
      "| epoch   9 |   800/ 1270 batches | lr 0.00 | ms/batch 31.70 | loss 0.016 | ppl    1.016\n",
      "| epoch   9 |   900/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.020 | ppl    1.020\n",
      "| epoch   9 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.49 | loss 0.022 | ppl    1.023\n",
      "| epoch   9 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.99 | loss 0.019 | ppl    1.019\n",
      "| epoch   9 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.46 | loss 0.013 | ppl    1.013\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 42.34s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   100/ 1270 batches | lr 0.00 | ms/batch 31.05 | loss 0.024 | ppl    1.025\n",
      "| epoch  10 |   200/ 1270 batches | lr 0.00 | ms/batch 30.86 | loss 0.023 | ppl    1.023\n",
      "| epoch  10 |   300/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.020 | ppl    1.020\n",
      "| epoch  10 |   400/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.021 | ppl    1.021\n",
      "| epoch  10 |   500/ 1270 batches | lr 0.00 | ms/batch 30.74 | loss 0.016 | ppl    1.016\n",
      "| epoch  10 |   600/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.019 | ppl    1.019\n",
      "| epoch  10 |   700/ 1270 batches | lr 0.00 | ms/batch 31.13 | loss 0.019 | ppl    1.019\n",
      "| epoch  10 |   800/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.015 | ppl    1.016\n",
      "| epoch  10 |   900/ 1270 batches | lr 0.00 | ms/batch 30.76 | loss 0.022 | ppl    1.022\n",
      "| epoch  10 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.07 | loss 0.021 | ppl    1.021\n",
      "| epoch  10 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.01 | loss 0.017 | ppl    1.017\n",
      "| epoch  10 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.06 | loss 0.014 | ppl    1.014\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 41.96s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  11 |   100/ 1270 batches | lr 0.00 | ms/batch 31.25 | loss 0.023 | ppl    1.023\n",
      "| epoch  11 |   200/ 1270 batches | lr 0.00 | ms/batch 31.54 | loss 0.022 | ppl    1.022\n",
      "| epoch  11 |   300/ 1270 batches | lr 0.00 | ms/batch 31.31 | loss 0.020 | ppl    1.020\n",
      "| epoch  11 |   400/ 1270 batches | lr 0.00 | ms/batch 31.12 | loss 0.020 | ppl    1.021\n",
      "| epoch  11 |   500/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.018 | ppl    1.018\n",
      "| epoch  11 |   600/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.017 | ppl    1.017\n",
      "| epoch  11 |   700/ 1270 batches | lr 0.00 | ms/batch 31.07 | loss 0.017 | ppl    1.017\n",
      "| epoch  11 |   800/ 1270 batches | lr 0.00 | ms/batch 30.82 | loss 0.015 | ppl    1.016\n",
      "| epoch  11 |   900/ 1270 batches | lr 0.00 | ms/batch 31.42 | loss 0.020 | ppl    1.020\n",
      "| epoch  11 |  1000/ 1270 batches | lr 0.00 | ms/batch 31.18 | loss 0.019 | ppl    1.020\n",
      "| epoch  11 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.91 | loss 0.019 | ppl    1.019\n",
      "| epoch  11 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.28 | loss 0.013 | ppl    1.013\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 42.09s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  12 |   100/ 1270 batches | lr 0.00 | ms/batch 31.50 | loss 0.022 | ppl    1.022\n",
      "| epoch  12 |   200/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.022 | ppl    1.022\n",
      "| epoch  12 |   300/ 1270 batches | lr 0.00 | ms/batch 31.14 | loss 0.021 | ppl    1.021\n",
      "| epoch  12 |   400/ 1270 batches | lr 0.00 | ms/batch 30.87 | loss 0.020 | ppl    1.020\n",
      "| epoch  12 |   500/ 1270 batches | lr 0.00 | ms/batch 30.52 | loss 0.017 | ppl    1.017\n",
      "| epoch  12 |   600/ 1270 batches | lr 0.00 | ms/batch 30.96 | loss 0.018 | ppl    1.018\n",
      "| epoch  12 |   700/ 1270 batches | lr 0.00 | ms/batch 31.10 | loss 0.016 | ppl    1.016\n",
      "| epoch  12 |   800/ 1270 batches | lr 0.00 | ms/batch 30.92 | loss 0.016 | ppl    1.016\n",
      "| epoch  12 |   900/ 1270 batches | lr 0.00 | ms/batch 31.06 | loss 0.021 | ppl    1.021\n",
      "| epoch  12 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.82 | loss 0.021 | ppl    1.021\n",
      "| epoch  12 |  1100/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.018 | ppl    1.018\n",
      "| epoch  12 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.18 | loss 0.013 | ppl    1.014\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 41.87s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  13 |   100/ 1270 batches | lr 0.00 | ms/batch 31.41 | loss 0.024 | ppl    1.024\n",
      "| epoch  13 |   200/ 1270 batches | lr 0.00 | ms/batch 30.83 | loss 0.022 | ppl    1.022\n",
      "| epoch  13 |   300/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.019 | ppl    1.019\n",
      "| epoch  13 |   400/ 1270 batches | lr 0.00 | ms/batch 31.08 | loss 0.020 | ppl    1.021\n",
      "| epoch  13 |   500/ 1270 batches | lr 0.00 | ms/batch 30.94 | loss 0.016 | ppl    1.017\n",
      "| epoch  13 |   600/ 1270 batches | lr 0.00 | ms/batch 30.74 | loss 0.019 | ppl    1.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  13 |   700/ 1270 batches | lr 0.00 | ms/batch 31.08 | loss 0.018 | ppl    1.018\n",
      "| epoch  13 |   800/ 1270 batches | lr 0.00 | ms/batch 31.09 | loss 0.016 | ppl    1.017\n",
      "| epoch  13 |   900/ 1270 batches | lr 0.00 | ms/batch 30.81 | loss 0.021 | ppl    1.022\n",
      "| epoch  13 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.71 | loss 0.019 | ppl    1.020\n",
      "| epoch  13 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.68 | loss 0.017 | ppl    1.018\n",
      "| epoch  13 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.013 | ppl    1.013\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 41.84s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 |   100/ 1270 batches | lr 0.00 | ms/batch 31.61 | loss 0.023 | ppl    1.023\n",
      "| epoch  14 |   200/ 1270 batches | lr 0.00 | ms/batch 30.86 | loss 0.021 | ppl    1.021\n",
      "| epoch  14 |   300/ 1270 batches | lr 0.00 | ms/batch 31.39 | loss 0.019 | ppl    1.019\n",
      "| epoch  14 |   400/ 1270 batches | lr 0.00 | ms/batch 30.83 | loss 0.020 | ppl    1.020\n",
      "| epoch  14 |   500/ 1270 batches | lr 0.00 | ms/batch 30.93 | loss 0.017 | ppl    1.017\n",
      "| epoch  14 |   600/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.016 | ppl    1.016\n",
      "| epoch  14 |   700/ 1270 batches | lr 0.00 | ms/batch 31.36 | loss 0.017 | ppl    1.018\n",
      "| epoch  14 |   800/ 1270 batches | lr 0.00 | ms/batch 30.84 | loss 0.016 | ppl    1.016\n",
      "| epoch  14 |   900/ 1270 batches | lr 0.00 | ms/batch 30.90 | loss 0.021 | ppl    1.021\n",
      "| epoch  14 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.88 | loss 0.021 | ppl    1.022\n",
      "| epoch  14 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.74 | loss 0.018 | ppl    1.018\n",
      "| epoch  14 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.91 | loss 0.013 | ppl    1.013\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 41.91s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 |   100/ 1270 batches | lr 0.00 | ms/batch 31.59 | loss 0.022 | ppl    1.022\n",
      "| epoch  15 |   200/ 1270 batches | lr 0.00 | ms/batch 30.62 | loss 0.020 | ppl    1.021\n",
      "| epoch  15 |   300/ 1270 batches | lr 0.00 | ms/batch 31.03 | loss 0.021 | ppl    1.021\n",
      "| epoch  15 |   400/ 1270 batches | lr 0.00 | ms/batch 31.12 | loss 0.021 | ppl    1.021\n",
      "| epoch  15 |   500/ 1270 batches | lr 0.00 | ms/batch 30.82 | loss 0.017 | ppl    1.017\n",
      "| epoch  15 |   600/ 1270 batches | lr 0.00 | ms/batch 31.04 | loss 0.017 | ppl    1.017\n",
      "| epoch  15 |   700/ 1270 batches | lr 0.00 | ms/batch 30.88 | loss 0.017 | ppl    1.017\n",
      "| epoch  15 |   800/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.016 | ppl    1.016\n",
      "| epoch  15 |   900/ 1270 batches | lr 0.00 | ms/batch 31.10 | loss 0.020 | ppl    1.020\n",
      "| epoch  15 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.70 | loss 0.019 | ppl    1.019\n",
      "| epoch  15 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.77 | loss 0.018 | ppl    1.018\n",
      "| epoch  15 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.14 | loss 0.012 | ppl    1.012\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 41.85s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  16 |   100/ 1270 batches | lr 0.00 | ms/batch 31.50 | loss 0.022 | ppl    1.022\n",
      "| epoch  16 |   200/ 1270 batches | lr 0.00 | ms/batch 30.61 | loss 0.022 | ppl    1.023\n",
      "| epoch  16 |   300/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.020 | ppl    1.020\n",
      "| epoch  16 |   400/ 1270 batches | lr 0.00 | ms/batch 30.98 | loss 0.020 | ppl    1.020\n",
      "| epoch  16 |   500/ 1270 batches | lr 0.00 | ms/batch 30.38 | loss 0.017 | ppl    1.018\n",
      "| epoch  16 |   600/ 1270 batches | lr 0.00 | ms/batch 30.57 | loss 0.015 | ppl    1.015\n",
      "| epoch  16 |   700/ 1270 batches | lr 0.00 | ms/batch 31.19 | loss 0.017 | ppl    1.017\n",
      "| epoch  16 |   800/ 1270 batches | lr 0.00 | ms/batch 30.71 | loss 0.017 | ppl    1.017\n",
      "| epoch  16 |   900/ 1270 batches | lr 0.00 | ms/batch 30.48 | loss 0.020 | ppl    1.020\n",
      "| epoch  16 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.74 | loss 0.019 | ppl    1.020\n",
      "| epoch  16 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.94 | loss 0.019 | ppl    1.019\n",
      "| epoch  16 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.92 | loss 0.012 | ppl    1.012\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 41.67s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |   100/ 1270 batches | lr 0.00 | ms/batch 31.31 | loss 0.023 | ppl    1.023\n",
      "| epoch  17 |   200/ 1270 batches | lr 0.00 | ms/batch 30.68 | loss 0.021 | ppl    1.021\n",
      "| epoch  17 |   300/ 1270 batches | lr 0.00 | ms/batch 30.95 | loss 0.019 | ppl    1.019\n",
      "| epoch  17 |   400/ 1270 batches | lr 0.00 | ms/batch 30.60 | loss 0.020 | ppl    1.020\n",
      "| epoch  17 |   500/ 1270 batches | lr 0.00 | ms/batch 30.70 | loss 0.017 | ppl    1.018\n",
      "| epoch  17 |   600/ 1270 batches | lr 0.00 | ms/batch 30.68 | loss 0.017 | ppl    1.017\n",
      "| epoch  17 |   700/ 1270 batches | lr 0.00 | ms/batch 30.98 | loss 0.017 | ppl    1.017\n",
      "| epoch  17 |   800/ 1270 batches | lr 0.00 | ms/batch 30.91 | loss 0.015 | ppl    1.015\n",
      "| epoch  17 |   900/ 1270 batches | lr 0.00 | ms/batch 30.75 | loss 0.020 | ppl    1.021\n",
      "| epoch  17 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.81 | loss 0.019 | ppl    1.019\n",
      "| epoch  17 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.93 | loss 0.018 | ppl    1.018\n",
      "| epoch  17 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.74 | loss 0.011 | ppl    1.012\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 41.69s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 |   100/ 1270 batches | lr 0.00 | ms/batch 31.55 | loss 0.022 | ppl    1.023\n",
      "| epoch  18 |   200/ 1270 batches | lr 0.00 | ms/batch 30.56 | loss 0.021 | ppl    1.021\n",
      "| epoch  18 |   300/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.020 | ppl    1.020\n",
      "| epoch  18 |   400/ 1270 batches | lr 0.00 | ms/batch 30.86 | loss 0.021 | ppl    1.021\n",
      "| epoch  18 |   500/ 1270 batches | lr 0.00 | ms/batch 30.65 | loss 0.017 | ppl    1.017\n",
      "| epoch  18 |   600/ 1270 batches | lr 0.00 | ms/batch 30.83 | loss 0.016 | ppl    1.016\n",
      "| epoch  18 |   700/ 1270 batches | lr 0.00 | ms/batch 31.17 | loss 0.017 | ppl    1.018\n",
      "| epoch  18 |   800/ 1270 batches | lr 0.00 | ms/batch 30.79 | loss 0.015 | ppl    1.015\n",
      "| epoch  18 |   900/ 1270 batches | lr 0.00 | ms/batch 30.64 | loss 0.021 | ppl    1.021\n",
      "| epoch  18 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.88 | loss 0.019 | ppl    1.019\n",
      "| epoch  18 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.67 | loss 0.018 | ppl    1.018\n",
      "| epoch  18 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.12 | loss 0.012 | ppl    1.012\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 41.88s | valid loss 0.006 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  19 |   100/ 1270 batches | lr 0.00 | ms/batch 31.93 | loss 0.022 | ppl    1.022\n",
      "| epoch  19 |   200/ 1270 batches | lr 0.00 | ms/batch 30.94 | loss 0.019 | ppl    1.020\n",
      "| epoch  19 |   300/ 1270 batches | lr 0.00 | ms/batch 31.11 | loss 0.018 | ppl    1.018\n",
      "| epoch  19 |   400/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.019 | ppl    1.019\n",
      "| epoch  19 |   500/ 1270 batches | lr 0.00 | ms/batch 30.47 | loss 0.017 | ppl    1.017\n",
      "| epoch  19 |   600/ 1270 batches | lr 0.00 | ms/batch 30.89 | loss 0.017 | ppl    1.017\n",
      "| epoch  19 |   700/ 1270 batches | lr 0.00 | ms/batch 30.93 | loss 0.016 | ppl    1.017\n",
      "| epoch  19 |   800/ 1270 batches | lr 0.00 | ms/batch 30.51 | loss 0.015 | ppl    1.015\n",
      "| epoch  19 |   900/ 1270 batches | lr 0.00 | ms/batch 30.97 | loss 0.020 | ppl    1.020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  19 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.99 | loss 0.019 | ppl    1.020\n",
      "| epoch  19 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.79 | loss 0.018 | ppl    1.018\n",
      "| epoch  19 |  1200/ 1270 batches | lr 0.00 | ms/batch 31.16 | loss 0.013 | ppl    1.013\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 41.86s | valid loss 0.007 | valid ppl    1.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |   100/ 1270 batches | lr 0.00 | ms/batch 31.20 | loss 0.023 | ppl    1.023\n",
      "| epoch  20 |   200/ 1270 batches | lr 0.00 | ms/batch 30.67 | loss 0.021 | ppl    1.021\n",
      "| epoch  20 |   300/ 1270 batches | lr 0.00 | ms/batch 31.28 | loss 0.018 | ppl    1.019\n",
      "| epoch  20 |   400/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.020 | ppl    1.020\n",
      "| epoch  20 |   500/ 1270 batches | lr 0.00 | ms/batch 30.58 | loss 0.017 | ppl    1.017\n",
      "| epoch  20 |   600/ 1270 batches | lr 0.00 | ms/batch 30.51 | loss 0.016 | ppl    1.016\n",
      "| epoch  20 |   700/ 1270 batches | lr 0.00 | ms/batch 30.77 | loss 0.018 | ppl    1.018\n",
      "| epoch  20 |   800/ 1270 batches | lr 0.00 | ms/batch 30.25 | loss 0.016 | ppl    1.016\n",
      "| epoch  20 |   900/ 1270 batches | lr 0.00 | ms/batch 31.00 | loss 0.019 | ppl    1.019\n",
      "| epoch  20 |  1000/ 1270 batches | lr 0.00 | ms/batch 30.97 | loss 0.019 | ppl    1.019\n",
      "| epoch  20 |  1100/ 1270 batches | lr 0.00 | ms/batch 30.85 | loss 0.019 | ppl    1.019\n",
      "| epoch  20 |  1200/ 1270 batches | lr 0.00 | ms/batch 30.79 | loss 0.012 | ppl    1.012\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 41.77s | valid loss 0.006 | valid ppl    1.006\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 20 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    val_loss = evaluate(model)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.3f} | '\n",
    "          'valid ppl {:8.3f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "improved-developer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8510657 0.868\n",
      "0.99176854 0.954\n",
      "0.8503028 0.851\n",
      "0.8623636 0.766\n",
      "0.8887348 0.961\n",
      "0.8396364 0.633\n",
      "0.8843679 0.766\n",
      "0.93044865 0.961\n",
      "0.8563968 0.907\n",
      "0.8071856 0.992\n",
      "MAE: 0.08150021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "true = []\n",
    "pred = []\n",
    "for i in range(10):\n",
    "    a,b = get_batch(i, xvalid, yvalid)\n",
    "    b = b[0].cpu().detach().numpy()\n",
    "    ev = model(a)[0][0].cpu().detach().numpy()\n",
    "    print(ev, b)\n",
    "    true.append(b)\n",
    "    pred.append(ev)\n",
    "print(\"MAE:\", mean_absolute_error(true, pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
