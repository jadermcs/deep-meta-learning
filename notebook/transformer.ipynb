{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "funny-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "swedish-accessory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guided-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "worse-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improving-constraint",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = 2000 # the size of vocabulary\n",
    "emsize = 200 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "joined-carry",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.randint(0, 1, (10, 5)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ordered-eight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.6545,  0.5618,  1.2052,  ...,  1.1348,  2.2492, -0.3276],\n",
       "         [-0.1621,  0.4525,  2.1023,  ...,  0.3499,  2.0883, -0.1150],\n",
       "         [-0.3876,  0.9137,  0.3812,  ...,  0.7751,  1.8150, -0.8984],\n",
       "         [ 0.0819,  1.0205,  1.7666,  ...,  0.6029,  1.6366, -0.3055],\n",
       "         [-0.3692,  0.5628,  1.0247,  ...,  0.7533,  2.3122, -0.4255]],\n",
       "\n",
       "        [[-0.2824,  0.0696,  0.7567,  ...,  2.0970,  0.9256, -0.3983],\n",
       "         [-0.2628,  0.0676,  1.0979,  ...,  1.0309,  2.1973, -0.6835],\n",
       "         [-0.6189,  0.5154,  0.8990,  ...,  1.4285,  2.0477, -0.9060],\n",
       "         [-0.5621,  0.6226,  1.6725,  ...,  1.1949,  2.2357, -0.9005],\n",
       "         [ 0.2564,  0.5485,  0.9278,  ...,  0.6419,  2.8465, -1.1684]],\n",
       "\n",
       "        [[-0.5603,  0.3696,  0.8197,  ...,  0.7508,  1.9138, -1.1334],\n",
       "         [-0.0894,  0.1941,  1.4274,  ...,  0.7642,  2.2985, -1.1003],\n",
       "         [-0.7572,  0.3337,  0.9974,  ...,  0.8834,  1.9280, -1.0057],\n",
       "         [-0.6904,  0.1039,  0.8335,  ...,  1.2978,  1.3635, -0.5116],\n",
       "         [-0.1314,  0.4351,  0.7892,  ...,  0.9579,  1.8261, -0.7464]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.5892,  0.4825, -0.2362,  ...,  1.2833,  1.7871, -0.6022],\n",
       "         [-0.5749,  0.6287,  0.9359,  ...,  0.2732,  2.6532, -0.3454],\n",
       "         [-0.4020,  0.7324,  1.1060,  ..., -0.0404,  2.2423, -1.1794],\n",
       "         [-0.9685,  0.1800,  0.5569,  ...,  0.9259,  1.8373, -0.2001],\n",
       "         [-0.0445, -0.0987,  0.5846,  ...,  0.9003,  1.5337, -1.1918]],\n",
       "\n",
       "        [[-0.7706, -0.3951,  0.7267,  ...,  0.9357,  2.1557, -0.3445],\n",
       "         [ 0.0082,  0.4122,  0.2208,  ...,  0.4149,  2.4590, -0.8691],\n",
       "         [-0.3482,  0.2400,  1.0740,  ...,  0.5884,  1.5260, -0.8340],\n",
       "         [-0.7334,  0.5274,  0.5113,  ...,  0.7874,  2.0596, -0.0399],\n",
       "         [-0.2687,  0.7170,  0.7303,  ...,  0.5416,  1.9070, -0.5892]],\n",
       "\n",
       "        [[-0.6203,  0.2142, -0.3489,  ...,  1.1113,  2.0017, -0.5181],\n",
       "         [-0.0818,  0.4065,  0.8928,  ...,  0.1163,  2.9599, -1.0193],\n",
       "         [-0.3308, -0.5036,  0.6314,  ...,  0.7763,  1.4025, -0.6656],\n",
       "         [ 0.0041,  0.4920,  0.4239,  ...,  0.9283,  2.7829,  0.2791],\n",
       "         [-0.6431,  0.4159,  0.7240,  ...,  0.9205,  2.1955, -0.5436]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-extent",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
